{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87e3883",
   "metadata": {},
   "source": [
    "# 3. The current Jupyter Notebook will cover the Third Phase of the project \"Test\"\n",
    "\n",
    "One of the critical metrics used in the current phase of the project is ROUGE. The mathematical basis for using this metric to compare a computer-generated summary against a human reference is detailed in the paper \"ROUGE: A Package for Automatic Evaluation of Summaries\" (Lin, 2004).\n",
    "\n",
    "Specifically, this framework allows us to quantify the quality of the generation by measuring the n-gram overlap (shared words and phrases) between the system's output and the ground truth.\n",
    "\n",
    "## 3.1 Evaluate retrieval quality using Precision@k and Top-k Accuracy.\n",
    "\n",
    "The metrics used to evaluate the quality of the answers are as follows:\n",
    "\n",
    "- Precision@k: This metric measures, out of the k retrieved chunks, how many are actually relevant to answering the question.\n",
    "- Top‑k Accuracy: This metric measures whether at least one relevant chunk appears within the top k results.\n",
    "\n",
    "Applying Precision@k requires manual evaluation, since a human must determine whether the retrieved chunks truly contain relevant information to answer the question. In contrast, to apply the Top‑k Accuracy metric, we will define a test set with two questions and the expected chapters where the relevant chunks should be found. A function will then be implemented to compute whether at least one of the top k retrieved chunks comes from the expected chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029d38f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Define the database where the search is performed\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the path of the database\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbc7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Top-K accuracy test!\n",
      "The question 'Who killed Rom?' was not found in 'arc-1-chapter-11.txt'.\n",
      "The retrieved sources were: ['arc-1-chapter-9.txt', 'arc-1-chapter-21.txt', 'arc-1-chapter-21.txt']\n",
      "And the retrieved chunks were:\n",
      "Chunk 1: Rom’s face was stern as he answered Subaru’s tactless question.\n",
      "\n",
      "He then brought the bottle he had been pouring out of to his mouth, and as he drank,\n",
      "\n",
      "“Because of this, most of us were wiped out. Even in the capital, I haven’t seen any other giants.”\n",
      "\n",
      "“Yer strong even without eatin’, sho kewl. … Gunna throw up.”\n",
      "\n",
      "“I’m saying something sad here and you respond like that?”\n",
      "\n",
      "He wasn’t about to let someone’s sob story kill his mood.\n",
      "\n",
      "As Subaru blocked his ears and interrupted the story, Rom gave up on telling it and started eating his beans.\n",
      "\n",
      "The two of them passed their time silently eating those terrible beans as a side to their alcohol.\n",
      "\n",
      "Eventually there was a coded knock on the door, by which time the sun had already set for the most part.\n",
      "\n",
      "Subaru who had been nodding off raised his head, and Rom nimbly approached the door in response to the sound.\n",
      "Chunk 2: Sighting a rundown shack, she ran up the wall and stood atop it, she then sent spittle flying as she screamed.\n",
      "\n",
      "It wasn’t just anyone that she needed.\n",
      "\n",
      "She needed someone who had the power to face that murderer―Elsa.\n",
      "\n",
      "If anyone would do, she had already seen several figures as she ran here. But Felt was by no means someone well-liked around here.\n",
      "\n",
      "Especially now that she was in trouble, Rom was probably the only one around here who would help her.\n",
      "\n",
      "If she just left Rom like this, he would die.\n",
      "\n",
      "Even Subaru who had helped Felt escape was probably fighting for his life right now.\n",
      "\n",
      "It was a strange feeling. Without understanding why, she desperately rubbed her eyes that were on the verge of tears.\n",
      "\n",
      "Setting Rom aside, what was so bad about some guy she just met dying? He seemed to have had a good upbringing, spoke a lot of nonsense, and to call him inconsiderate was putting it lightly.\n",
      "Chunk 3: He approached Rom’s fallen body, and somehow managed to drag him near the wall.\n",
      "\n",
      "“Old man, Rom. Hey, baldy, listen here, are you still alive?”\n",
      "\n",
      "“Uu… Who’s… Bald…”\n",
      "\n",
      "“You, obviously. As if it’d be anyone else. My only goals in life are to never go bald or become fat. From my perspective, you’re a great example of what not to do.”\n",
      "\n",
      "Though frail, at least he was able to respond, Subaru struck his forehead and heaved a sigh of relief.\n",
      "\n",
      "Setting aside the severe damage to his head, the old man was probably alright. His memory might end up getting a little worse, but that would be a small price to pay to keep his life.\n",
      "\n",
      "“Is that person alright?”\n",
      "\n",
      "As Subaru relaxed, Not-Satella called out while running over to him.\n",
      "\n",
      "With her long silver hair flowing behind her, she analyzed Rom’s injuries, she then whispered, ‘I’ll have to heal this’ as a faint blue glow emanated from her hands.\n",
      "\n",
      "“Hey now, this old man’s one of the people responsible for your insignia’s theft, you know?”\n",
      "The question 'What was the object Felt stole to Satella/The silver-haired girl?' was correctly found in 'arc-1-chapter-8.txt'\n",
      "The retrieved sources were: ['arc-1-chapter-8.txt', 'arc-1-chapter-15.txt', 'arc-1-chapter-14.txt']\n",
      "And the retrieved chunks were:\n",
      "Chunk 1: “The one bringing it in… Perhaps a girl named Felt?”\n",
      "\n",
      "“What, you even know who stole it?”\n",
      "\n",
      "Rom seemed to think this was anticlimactic, but Subaru couldn’t help but strike a triumphant pose.\n",
      "\n",
      "The string of hope he thought for sure had snapped still held strong.\n",
      "\n",
      "He had just confirmed that Felt existed. If the girl who stole the insignia was real, then Satella, from whom it was stolen, should also be real.\n",
      "\n",
      "“I was wondering if my love for silver-haired heroines was making me see things, it freaked me out…”\n",
      "\n",
      "“Sorry to rain on your parade, but whether or not you’ll be able to buy it once she brings it here is a different matter. A jeweled insignia would have a price befitting it, no?”\n",
      "\n",
      "“Ha! It’s no use trying to size me up, I’m flat broke!”\n",
      "\n",
      "“There’s nothing to talk about then!”\n",
      "\n",
      "Perhaps having wanted to start negotiations early, Rom seemed disappointed as he yelled. However, Subaru waved his finger in response.\n",
      "Chunk 2: “I’m not well versed in the happenings of the world so I’m not sure if I’ll be able to answer, though.”\n",
      "\n",
      "“Well, what I’m asking about relates to a person I’m searching for, so that’s fine. So, have you seen a silver-haired girl in a white robe around here?”\n",
      "\n",
      "Having surveyed the main street several times, he could tell that Not-Satella’s clothes stood out. Her silver hair was rare much like Subaru’s black hair, and of course there was her robe with the hawk embroidered on it.\n",
      "\n",
      "Thinking back, that white robe seemed expensive in its own way. Considering she had an insignia with a jewel embedded in it, he guessed her lineage must have been a prestigious one.\n",
      "\n",
      "“A white robe and silver hair…”\n",
      "\n",
      "“In addition, she’s super gorgeous. And there’s this cat… Well, it’s not like she goes around showing it off. That’s all I can tell you about her, ring any bells?”\n",
      "Chunk 3: And then this kind girl let the useless fellow accompany her for his own self-satisfaction, and ended up dying a horrible death for it.\n",
      "\n",
      "“Thinking back to the third time, there’s a whole lot I learned. Or rather, it’d be pathetic if I didn’t learn, but I’m not that stupid.”\n",
      "\n",
      "“What are you saying now?”\n",
      "\n",
      "“There’s probably a pattern here. You could even call it fate. ―No matter how many times it repeats, this will keep happening. For instance…”\n",
      "\n",
      "Felt stole Satella’s insignia all three times.\n",
      "\n",
      "Rom had been killed in the loot house both the first and second time. Felt had probably died the first time too.\n",
      "\n",
      "If so, what happened to Satella who most likely arrived at the loot house the second time as well?\n",
      "\n",
      "She didn’t have a burden holding her back, would she have been able to win against Elsa?\n",
      "\n",
      "“I don’t know. And I probably won’t know. But there’s things I do know.”\n",
      "The Top-k accuracy of the current test is: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Define the questions and answer set to evaluate the system quality\n",
    "\n",
    "test_set = [{'question':'Who killed Rom?', 'expected_source': 'arc-1-chapter-11.txt'}, \n",
    "            {'question': 'What was the object Felt stole to Satella/The silver-haired girl?', 'expected_source': 'arc-1-chapter-8.txt'}]\n",
    "\n",
    "# Define the function to perform the Top-K Accuracy metric.\n",
    "\n",
    "def topk_accuracy(questions, k=3):\n",
    "    print(\"Starting Top-K accuracy test!\")\n",
    "\n",
    "    correct_retrievals = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for item in test_set:\n",
    "        query = item['question']\n",
    "        expected_answer = item['expected_source']\n",
    "\n",
    "        # Perform the search\n",
    "        search = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "        # Check if any of the top k chunks is trieved from the expected source\n",
    "        found = False\n",
    "        retrieved_sources = []\n",
    "        retrieved_chunks = []\n",
    "\n",
    "        for doc in search:\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            chunk = doc.page_content\n",
    "            retrieved_sources.append(source)\n",
    "            retrieved_chunks.append(chunk)\n",
    "\n",
    "            if source == expected_answer:\n",
    "                found = True\n",
    "\n",
    "        if found == True:\n",
    "            correct_retrievals += 1\n",
    "            print(f\"The question '{query}' was correctly found in '{expected_answer}'\")\n",
    "            print(f\"The retrieved sources were: {retrieved_sources}\")\n",
    "            print(f\"And the retrieved chunks were:\")\n",
    "            print(f\"Chunk 1: {retrieved_chunks[0]}\")\n",
    "            print(f\"Chunk 2: {retrieved_chunks[1]}\")\n",
    "            print(f\"Chunk 3: {retrieved_chunks[2]}\")\n",
    "        else:\n",
    "            print(f\"The question '{query}' was not found in '{expected_answer}'.\")\n",
    "            print(f\"The retrieved sources were: {retrieved_sources}\")\n",
    "            print(f\"And the retrieved chunks were:\")\n",
    "            print(f\"Chunk 1: {retrieved_chunks[0]}\")\n",
    "            print(f\"Chunk 2: {retrieved_chunks[1]}\")\n",
    "            print(f\"Chunk 3: {retrieved_chunks[2]}\")\n",
    "\n",
    "    # Calculate the Top-k Accuracy\n",
    "\n",
    "    accuracy = (correct_retrievals/total_questions)*100\n",
    "    print(f\"The Top-k accuracy of the current test is: {accuracy}\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "\n",
    "topk_accuracy(test_set, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56f4e9",
   "metadata": {},
   "source": [
    "### About the Precision@k for the question 'Who killed Rom?'\n",
    "\n",
    "The search retrieved chunks from chapters 9 and 21 of Arc 1, but none of them contained relevant information confirming that Rom was killed by Elsa. Since no relevant evidence was found, and following the formula \"(# relevant chunks in top k / k)*100\", the Precision@k for this question is 0% ((0/3)*100).\n",
    "\n",
    "### About the Precision@k for the question 'What was the object Felt stole to Satella/The silver-haired girl?'\n",
    "\n",
    "The search retrieved chunks from chapters 8, 14, and 15, and all of them contained relevant information confirming that the object stolen by Felt from Satella (as Emilia was referred to at this point in the novel) was an insignia.\n",
    "\n",
    "- In chapter 8, it is explicitly stated that Felt stole an insignia from Emilia (called Satella).\n",
    "- In chapter 14, the same fact is repeated.\n",
    "- In chapter 15, the text again mentions that the silver‑haired girl had an insignia stolen from her.\n",
    "\n",
    "Based on this evidence, the Precision@k for this question is 100% ((3/3)*100).\n",
    "\n",
    "## 3.2 Measure generative quality and coherence with ROUGE and cosine similarity between retrieved and generated text.\n",
    "\n",
    "ROUGE stands for “Recall‑Oriented Understudy for Gisting Evaluation” and is used to assess whether the final answers provided by the AI agent are correct and align with what a human would say.\n",
    "\n",
    "To apply this metric, we will use the 'evaluate' library along with the test_set variable. In each dictionary of the test set, we will add a new key called human_expected_answer, which will contain the answer to the question as a human would provide it. Additionally, a new function will be created to compute the ROUGE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdf167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ROUGE metric\n",
      "Searching in the vector database for the question Who killed Rom?\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: Who killed Rom?\n",
      "Answer: I don't know. The text doesn't mention who killed Rom, it only shows Subaru and Not-Satella trying to help him after he's been injured.\n",
      "==============================\n",
      "Searching in the vector database for the question What was the object Felt stole to Satella/The silver-haired girl?\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: What was the object Felt stole to Satella/The silver-haired girl?\n",
      "Answer: The object Felt stole from Satella was a jeweled insignia.\n",
      "==============================\n",
      "The ROUGE score is:\n",
      "{'rouge1': np.float64(0.41428571428571426), 'rouge2': np.float64(0.11616161616161616), 'rougeL': np.float64(0.41428571428571426), 'rougeLsum': np.float64(0.41428571428571426)}\n"
     ]
    }
   ],
   "source": [
    "# Add the new key to both dictionaries\n",
    "\n",
    "test_set[0].update({'human_expected_answer': 'Elsa was the one who killed Rom.'})\n",
    "test_set[1].update({'human_expected_answer': 'The object stolen by Felt from Emilia was an insignia.'})\n",
    "\n",
    "# Define the function to use ROUGE metric\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Load the ROUGE metric\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define the function to use the ROUGE metric\n",
    "\n",
    "def calculate_rouge(test_set):\n",
    "    print(\"Starting ROUGE metric\")\n",
    "\n",
    "    # ROUGE metric needs two list:\n",
    "    predictions = [] # One with the strings generated by the AI-agent\n",
    "    references = [] # One with the \"human\" answers\n",
    "\n",
    "    # Extract the needed information\n",
    "\n",
    "    for item in test_set:\n",
    "        query = item['question']\n",
    "        human_answer = item['human_expected_answer']\n",
    "\n",
    "        references.append(human_answer)\n",
    "\n",
    "        # Ask the AI-Agent the question and add the answer to the predictions list\n",
    "\n",
    "        from RAG_Module import query_question\n",
    "\n",
    "        predictions.append(query_question(query))\n",
    "\n",
    "    # Compute the score\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    print(f\"The ROUGE score is:\")\n",
    "    print(rouge_score)\n",
    "\n",
    "calculate_rouge(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc17f32",
   "metadata": {},
   "source": [
    "### ROUGE interpretation\n",
    "\n",
    "The current ROUGE results show that our AI agent answering in a 'human-like' behavior because the values are between 0.4 and 0.3 (which is considered a good score for prototypes).\n",
    "\n",
    "The values of the current run are: {'rouge1': np.float64(0.41428571428571426), 'rouge2': np.float64(0.11616161616161616), 'rougeL': np.float64(0.41428571428571426), 'rougeLsum': np.float64(0.41428571428571426)}\n",
    "\n",
    "However, in essence, the answer provided by the agent is correct. The reason the scores are too low is that the current prompt is not limiting the agent's answer, due to the general instruction provided which means that a redefinition of the prompt could be a possible solution to improve the ROUGE scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
