{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87e3883",
   "metadata": {},
   "source": [
    "# 3. The current Jupyter Notebook will cover the Third Phase of the project \"Test\"\n",
    "\n",
    "## 3.1 Evaluate retrieval quality using Precision@k and Top-k Accuracy.\n",
    "\n",
    "The metrics used to evaluate the quality of the answers are as follows:\n",
    "\n",
    "- Precision@k: This metric measures, out of the k retrieved chunks, how many are actually relevant to answering the question.\n",
    "- Top‑k Accuracy: This metric measures whether at least one relevant chunk appears within the top k results.\n",
    "\n",
    "Applying Precision@k requires manual evaluation, since a human must determine whether the retrieved chunks truly contain relevant information to answer the question. In contrast, to apply the Top‑k Accuracy metric, we will define a test set with two questions and the expected chapters where the relevant chunks should be found. A function will then be implemented to compute whether at least one of the top k retrieved chunks comes from the expected chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029d38f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Define the database where the search is performed\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the path of the database\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbc7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Top-K accuracy test!\n",
      "The question 'Who killed Rom?' was not found in 'arc-1-chapter-11.txt'.\n",
      "The retrieved sources were: ['arc-1-chapter-9.txt', 'arc-1-chapter-21.txt', 'arc-1-chapter-21.txt']\n",
      "And the retrieved chunks were:\n",
      "Chunk 1: Rom’s face was stern as he answered Subaru’s tactless question.\n",
      "\n",
      "He then brought the bottle he had been pouring out of to his mouth, and as he drank,\n",
      "\n",
      "“Because of this, most of us were wiped out. Even in the capital, I haven’t seen any other giants.”\n",
      "\n",
      "“Yer strong even without eatin’, sho kewl. … Gunna throw up.”\n",
      "\n",
      "“I’m saying something sad here and you respond like that?”\n",
      "\n",
      "He wasn’t about to let someone’s sob story kill his mood.\n",
      "\n",
      "As Subaru blocked his ears and interrupted the story, Rom gave up on telling it and started eating his beans.\n",
      "\n",
      "The two of them passed their time silently eating those terrible beans as a side to their alcohol.\n",
      "\n",
      "Eventually there was a coded knock on the door, by which time the sun had already set for the most part.\n",
      "\n",
      "Subaru who had been nodding off raised his head, and Rom nimbly approached the door in response to the sound.\n",
      "Chunk 2: Sighting a rundown shack, she ran up the wall and stood atop it, she then sent spittle flying as she screamed.\n",
      "\n",
      "It wasn’t just anyone that she needed.\n",
      "\n",
      "She needed someone who had the power to face that murderer―Elsa.\n",
      "\n",
      "If anyone would do, she had already seen several figures as she ran here. But Felt was by no means someone well-liked around here.\n",
      "\n",
      "Especially now that she was in trouble, Rom was probably the only one around here who would help her.\n",
      "\n",
      "If she just left Rom like this, he would die.\n",
      "\n",
      "Even Subaru who had helped Felt escape was probably fighting for his life right now.\n",
      "\n",
      "It was a strange feeling. Without understanding why, she desperately rubbed her eyes that were on the verge of tears.\n",
      "\n",
      "Setting Rom aside, what was so bad about some guy she just met dying? He seemed to have had a good upbringing, spoke a lot of nonsense, and to call him inconsiderate was putting it lightly.\n",
      "Chunk 3: He approached Rom’s fallen body, and somehow managed to drag him near the wall.\n",
      "\n",
      "“Old man, Rom. Hey, baldy, listen here, are you still alive?”\n",
      "\n",
      "“Uu… Who’s… Bald…”\n",
      "\n",
      "“You, obviously. As if it’d be anyone else. My only goals in life are to never go bald or become fat. From my perspective, you’re a great example of what not to do.”\n",
      "\n",
      "Though frail, at least he was able to respond, Subaru struck his forehead and heaved a sigh of relief.\n",
      "\n",
      "Setting aside the severe damage to his head, the old man was probably alright. His memory might end up getting a little worse, but that would be a small price to pay to keep his life.\n",
      "\n",
      "“Is that person alright?”\n",
      "\n",
      "As Subaru relaxed, Not-Satella called out while running over to him.\n",
      "\n",
      "With her long silver hair flowing behind her, she analyzed Rom’s injuries, she then whispered, ‘I’ll have to heal this’ as a faint blue glow emanated from her hands.\n",
      "\n",
      "“Hey now, this old man’s one of the people responsible for your insignia’s theft, you know?”\n",
      "The question 'What was the object Felt stole to Satella/The silver-haired girl?' was correctly found in 'arc-1-chapter-8.txt'\n",
      "The retrieved sources were: ['arc-1-chapter-8.txt', 'arc-1-chapter-15.txt', 'arc-1-chapter-14.txt']\n",
      "And the retrieved chunks were:\n",
      "Chunk 1: “The one bringing it in… Perhaps a girl named Felt?”\n",
      "\n",
      "“What, you even know who stole it?”\n",
      "\n",
      "Rom seemed to think this was anticlimactic, but Subaru couldn’t help but strike a triumphant pose.\n",
      "\n",
      "The string of hope he thought for sure had snapped still held strong.\n",
      "\n",
      "He had just confirmed that Felt existed. If the girl who stole the insignia was real, then Satella, from whom it was stolen, should also be real.\n",
      "\n",
      "“I was wondering if my love for silver-haired heroines was making me see things, it freaked me out…”\n",
      "\n",
      "“Sorry to rain on your parade, but whether or not you’ll be able to buy it once she brings it here is a different matter. A jeweled insignia would have a price befitting it, no?”\n",
      "\n",
      "“Ha! It’s no use trying to size me up, I’m flat broke!”\n",
      "\n",
      "“There’s nothing to talk about then!”\n",
      "\n",
      "Perhaps having wanted to start negotiations early, Rom seemed disappointed as he yelled. However, Subaru waved his finger in response.\n",
      "Chunk 2: “I’m not well versed in the happenings of the world so I’m not sure if I’ll be able to answer, though.”\n",
      "\n",
      "“Well, what I’m asking about relates to a person I’m searching for, so that’s fine. So, have you seen a silver-haired girl in a white robe around here?”\n",
      "\n",
      "Having surveyed the main street several times, he could tell that Not-Satella’s clothes stood out. Her silver hair was rare much like Subaru’s black hair, and of course there was her robe with the hawk embroidered on it.\n",
      "\n",
      "Thinking back, that white robe seemed expensive in its own way. Considering she had an insignia with a jewel embedded in it, he guessed her lineage must have been a prestigious one.\n",
      "\n",
      "“A white robe and silver hair…”\n",
      "\n",
      "“In addition, she’s super gorgeous. And there’s this cat… Well, it’s not like she goes around showing it off. That’s all I can tell you about her, ring any bells?”\n",
      "Chunk 3: And then this kind girl let the useless fellow accompany her for his own self-satisfaction, and ended up dying a horrible death for it.\n",
      "\n",
      "“Thinking back to the third time, there’s a whole lot I learned. Or rather, it’d be pathetic if I didn’t learn, but I’m not that stupid.”\n",
      "\n",
      "“What are you saying now?”\n",
      "\n",
      "“There’s probably a pattern here. You could even call it fate. ―No matter how many times it repeats, this will keep happening. For instance…”\n",
      "\n",
      "Felt stole Satella’s insignia all three times.\n",
      "\n",
      "Rom had been killed in the loot house both the first and second time. Felt had probably died the first time too.\n",
      "\n",
      "If so, what happened to Satella who most likely arrived at the loot house the second time as well?\n",
      "\n",
      "She didn’t have a burden holding her back, would she have been able to win against Elsa?\n",
      "\n",
      "“I don’t know. And I probably won’t know. But there’s things I do know.”\n",
      "The Top-k accuracy of the current test is: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Define the questions and answer set to evaluate the system quality\n",
    "\n",
    "test_set = [{'question':'Who killed Rom?', 'expected_source': 'arc-1-chapter-11.txt'}, \n",
    "            {'question': 'What was the object Felt stole to Satella/The silver-haired girl?', 'expected_source': 'arc-1-chapter-8.txt'}]\n",
    "\n",
    "# Define the function to perform the Top-K Accuracy metric.\n",
    "\n",
    "def topk_accuracy(questions, k=3):\n",
    "    print(\"Starting Top-K accuracy test!\")\n",
    "\n",
    "    correct_retrievals = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for item in test_set:\n",
    "        query = item['question']\n",
    "        expected_answer = item['expected_source']\n",
    "\n",
    "        # Perform the search\n",
    "        search = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "        # Check if any of the top k chunks is trieved from the expected source\n",
    "        found = False\n",
    "        retrieved_sources = []\n",
    "        retrieved_chunks = []\n",
    "\n",
    "        for doc in search:\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            chunk = doc.page_content\n",
    "            retrieved_sources.append(source)\n",
    "            retrieved_chunks.append(chunk)\n",
    "\n",
    "            if source == expected_answer:\n",
    "                found = True\n",
    "\n",
    "        if found == True:\n",
    "            correct_retrievals += 1\n",
    "            print(f\"The question '{query}' was correctly found in '{expected_answer}'\")\n",
    "            print(f\"The retrieved sources were: {retrieved_sources}\")\n",
    "            print(f\"And the retrieved chunks were:\")\n",
    "            print(f\"Chunk 1: {retrieved_chunks[0]}\")\n",
    "            print(f\"Chunk 2: {retrieved_chunks[1]}\")\n",
    "            print(f\"Chunk 3: {retrieved_chunks[2]}\")\n",
    "        else:\n",
    "            print(f\"The question '{query}' was not found in '{expected_answer}'.\")\n",
    "            print(f\"The retrieved sources were: {retrieved_sources}\")\n",
    "            print(f\"And the retrieved chunks were:\")\n",
    "            print(f\"Chunk 1: {retrieved_chunks[0]}\")\n",
    "            print(f\"Chunk 2: {retrieved_chunks[1]}\")\n",
    "            print(f\"Chunk 3: {retrieved_chunks[2]}\")\n",
    "\n",
    "    # Calculate the Top-k Accuracy\n",
    "\n",
    "    accuracy = (correct_retrievals/total_questions)*100\n",
    "    print(f\"The Top-k accuracy of the current test is: {accuracy}\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "\n",
    "topk_accuracy(test_set, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56f4e9",
   "metadata": {},
   "source": [
    "### About the Precision@k for the question 'Who killed Rom?'\n",
    "\n",
    "The search retrieved chunks from chapters 9 and 21 of Arc 1, but none of them contained relevant information confirming that Rom was killed by Elsa. Since no relevant evidence was found, and following the formula \"(# relevant chunks in top k / k)*100\", the Precision@k for this question is 0% ((0/3)*100).\n",
    "\n",
    "### About the Precision@k for the question 'What was the object Felt stole to Satella/The silver-haired girl?'\n",
    "\n",
    "The search retrieved chunks from chapters 8, 14, and 15, and all of them contained relevant information confirming that the object stolen by Felt from Satella (as Emilia was referred to at this point in the novel) was an insignia.\n",
    "\n",
    "- In chapter 8, it is explicitly stated that Felt stole an insignia from Emilia (called Satella).\n",
    "- In chapter 14, the same fact is repeated.\n",
    "- In chapter 15, the text again mentions that the silver‑haired girl had an insignia stolen from her.\n",
    "\n",
    "Based on this evidence, the Precision@k for this question is 100% ((3/3)*100).\n",
    "\n",
    "## 3.2 Measure generative quality and coherence with ROUGE and cosine similarity between retrieved and generated text.\n",
    "\n",
    "ROUGE stands for “Recall‑Oriented Understudy for Gisting Evaluation” and is used to assess whether the final answers provided by the AI agent are correct and align with what a human would say.\n",
    "\n",
    "To apply this metric, we will use the 'evaluate' library along with the test_set variable. In each dictionary of the test set, we will add a new key called human_expected_answer, which will contain the answer to the question as a human would provide it. Additionally, a new function will be created to compute the ROUGE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdf167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ROUGE metric\n",
      "Searching in the vector database for the question Who killed Rom?\n",
      "The LLM has been asked the question and is now generating the answer!\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "llama runner process has terminated: CUDA error (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe ROUGE score is:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(rouge_score)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mcalculate_rouge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcalculate_rouge\u001b[39m\u001b[34m(test_set)\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Ask the AI-Agent the question and add the answer to the predictions list\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRAG_Module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m query_question\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     predictions.append(\u001b[43mquery_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Compute the score\u001b[39;00m\n\u001b[32m     39\u001b[39m rouge_score = rouge.compute(predictions=predictions, references=references)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\RAG_Module.py:72\u001b[39m, in \u001b[36mquery_question\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_ollama\\chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\langchain_ollama\\chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\ollama\\_client.py:179\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    182\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: llama runner process has terminated: CUDA error (status code: 500)"
     ]
    }
   ],
   "source": [
    "# Add the new key to both dictionaries\n",
    "\n",
    "test_set[0].update({'human_expected_answer': 'Elsa was the one who killed Rom.'})\n",
    "test_set[1].update({'human_expected_answer': 'The object stolen by Felt from Emilia was an insignia.'})\n",
    "\n",
    "# Define the function to use ROUGE metric\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Load the ROUGE metric\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define the function to use the ROUGE metric\n",
    "\n",
    "def calculate_rouge(test_set):\n",
    "    print(\"Starting ROUGE metric\")\n",
    "\n",
    "    # ROUGE metric needs two list:\n",
    "    predictions = [] # One with the strings generated by the AI-agent\n",
    "    references = [] # One with the \"human\" answers\n",
    "\n",
    "    # Extract the needed information\n",
    "\n",
    "    for item in test_set:\n",
    "        query = item['question']\n",
    "        human_answer = item['human_expected_answer']\n",
    "\n",
    "        references.append(human_answer)\n",
    "\n",
    "        # Ask the AI-Agent the question and add the answer to the predictions list\n",
    "\n",
    "        from RAG_Module import query_question\n",
    "\n",
    "        predictions.append(query_question(query))\n",
    "\n",
    "    # Compute the score\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    print(f\"The ROUGE score is:\")\n",
    "    print(rouge_score)\n",
    "\n",
    "calculate_rouge(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc17f32",
   "metadata": {},
   "source": [
    "### ROUGE interpretation\n",
    "\n",
    "The current ROUGE results show that our AI agent is not answering in a 'human-like' behavior because they are less than 0.4 and 0.3 (which is considered a good score for prototypes).\n",
    "\n",
    "However, in essence, the answer provided by the agent is correct. The reason the scores are too low is that the current prompt is not limiting the agent's answer, due to the general instruction provided which means that a redefinition of the prompt could be a possible solution to improve the ROUGE scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
