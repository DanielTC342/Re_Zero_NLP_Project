{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7624b9e1",
   "metadata": {},
   "source": [
    "# 2. The current Jupyter Notebook will cover the Second Phase of the project \"RAG process\"\n",
    "\n",
    "According to the article \"What is retrieval augmented generation (RAG)?\" by IBM, RAG is \"an architecture for optimizing the performance of an artificial intelligence (AI) model by connecting it with external knowledge bases.\".\n",
    "\n",
    "The mechanics of this architecture are detailed in the seminal paper \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" (Lewis et al., 2020), which demonstrates that it is not necessary to retrain a massive AI model to teach it new facts.\n",
    "\n",
    "## 2.1 Develop the retrieval engine to identify narrative episodes based on vector similarity\n",
    "\n",
    "For this part of the project we will use Ollama to be the server of our RAG system so it will allow us to run LLM locally, therefore python will work as the \"client\".\n",
    "\n",
    "The current code will do the search based on the query and send it to Ollama to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18618ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration to use Ollama correctly created!\n",
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the configuration for the needed to use Ollama\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(\"Configuration to use Ollama correctly created!\")\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")\n",
    "\n",
    "# Connect to Ollama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "# Create a function that performs the vector similarity search\n",
    "\n",
    "def query_question(question):\n",
    "    \"\"\"The current function takes as the input a question in the Terminal\"\"\"\n",
    "\n",
    "    print(\"Searching in the vector database for the question {}\".format(question))\n",
    "\n",
    "    # Perform the search (Retrieve)\n",
    "\n",
    "    retrieves = vector_database.similarity_search(question, k=3) # K is the number of chunks the model will retrieve to answer the question.\n",
    "\n",
    "    if not retrieves:\n",
    "        print(\"No relevant information found with the current data stored in my memory. Try adding more data.\")\n",
    "        return\n",
    "    \n",
    "    # Print the chunks the search found after the question\n",
    "\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, retrieve in enumerate(retrieves):\n",
    "        print(f\"Chunk {i+1} extracted from {retrieve.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"and contains the text:\")\n",
    "        print(f\"{retrieve.page_content}\")\n",
    "    \n",
    "    # Combine the retrieved text into one context block\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([retrieve.page_content for retrieve in retrieves])\n",
    "\n",
    "    # Define the prompt to guide the answer (Augment)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant for the novel Re:Zero. \n",
    "    Use the following pieces of retrieved context to answer the user's question.\n",
    "    If the answer is not in the context, just say that you don't know.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask to the LLM (Generate)\n",
    "\n",
    "    print(\"The LLM has been asked the question and is now generating the answer!\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Print the result\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response.content}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce6e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in the vector database for the question Who stole Non-Stella's insignia?\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1 extracted from arc-1-chapter-17.txt\n",
      "and contains the text:\n",
      "Having reached this point, Subaru decided that beating around the bush would only worsen her impression of him.\n",
      "\n",
      "There was also the fact that Elsa was prowling around the area, so he wanted to finish this up as soon as possible. However, this caused Felt to become even more cautious.\n",
      "\n",
      "She placed her hand on her chest, presumably where she was keeping the insignia.\n",
      "\n",
      "“How do you know that I stole an insignia? I don’t imagine the one who requested it would have leaked it, and I only just stole it. It certainly isn’t the sort of thing you’d just happen to hear.”\n",
      "\n",
      "“Now that you mention it, that was far too impatient. I’m so damned careless!”\n",
      "\n",
      "“… You can’t possibly negotiate with anyone if you show your hand that easily. You crumbled way too easily to a bit of pressure.”\n",
      "\n",
      "As Subaru stressed over his thoughtless blunder, the hostility left Felt’s face.\n",
      "\n",
      "Subaru crouched down with his head in his knees and Felt knelt down to meet his gaze as she spoke, “Well?”\n",
      "Chunk 2 extracted from arc-1-chapter-4.txt\n",
      "and contains the text:\n",
      "The so-called insignia was probably something like those badges people like lawyers, prosecutors, members of the defense force etc. had to prove their identity.\n",
      "\n",
      "Unfortunately, Subaru had no memory whatsoever of seeing such a thing in the hour or so he was here.\n",
      "\n",
      "Back home, he had a veritable mountain of toy badges he’d collected as a child. But he didn’t know how to return there, and he’d probably get smashed by her ice if he tried offering her one.\n",
      "\n",
      "Consequently, his answer couldn’t possibly meet her expectations. But the girl simply nodded, not disappointed in the slightest.\n",
      "\n",
      "“I see. That’s that, then. But I did learn that you don’t know anything, so that shall suffice as compensation for healing you.”\n",
      "\n",
      "Her logic regarding her total loss would have shocked the sleaziest swindler. Leaving a dumbfounded Subaru behind, she loudly clapped her hands together as if she’d completely gotten over it.\n",
      "Chunk 3 extracted from arc-1-chapter-8.txt\n",
      "and contains the text:\n",
      "The jeweled insignia that had been stolen from Satella. She hadn’t told him the reason, but she was willing to confront any danger to retrieve it, that’s how important it was to her.\n",
      "\n",
      "If it had been brought into the slums, it definitely had to be in here, or so he was told.\n",
      "\n",
      "If the insignia at least existed, Subaru would know for sure he wasn’t dreaming.\n",
      "\n",
      "“An insignia with a jewel embedded in it… Sorry, but there’s nothing like that around here.”\n",
      "\n",
      "“… Are you sure? Try to remember, maybe you’re going senile?”\n",
      "\n",
      "“I’m in peak condition when I’ve got booze in me. If I can’t recall it now, I can only say I don’t know about it.”\n",
      "\n",
      "As it seemed like Subaru’s final hope had been ripped away, Rom smiled meaningfully.\n",
      "\n",
      "“I head there’s supposed to be a large haul coming in today. ― It could very well be this insignia you speak of.”\n",
      "\n",
      "“The one bringing it in… Perhaps a girl named Felt?”\n",
      "\n",
      "“What, you even know who stole it?”\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: Who stole Non-Stella's insignia?\n",
      "Answer: Felt stole Non-Stella's insignia.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk about Arc 1 (or type 'exit'): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        query_question(user_input) # I way to solve the CUDA error is to copy and paste the next line in your CMD: set OLLAMA_USE_CPU=true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
