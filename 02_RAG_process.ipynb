{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7624b9e1",
   "metadata": {},
   "source": [
    "# 2. The current Jupyter Notebook will cover the Second Phase of the project \"RAG process\"\n",
    "\n",
    "According to the article \"What is retrieval augmented generation (RAG)?\" by IBM, RAG is \"an architecture for optimizing the performance of an artificial intelligence (AI) model by connecting it with external knowledge bases.\".\n",
    "\n",
    "The mechanics of this architecture are detailed in the seminal paper \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" (Lewis et al., 2020), which demonstrates that it is not necessary to retrain a massive AI model to teach it new facts.\n",
    "\n",
    "## 2.1 Develop the retrieval engine to identify narrative episodes based on vector similarity\n",
    "\n",
    "For this part of the project we will use Ollama to be the server of our RAG system so it will allow us to run LLM locally, therefore python will work as the \"client\".\n",
    "\n",
    "The current code will do the search based on the query and send it to Ollama to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18618ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration to use Ollama correctly created!\n",
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the configuration for the needed to use Ollama\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(\"Configuration to use Ollama correctly created!\")\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")\n",
    "\n",
    "# Connect to Ollama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "# Create a function that performs the vector similarity search\n",
    "\n",
    "def query_question(question):\n",
    "    \"\"\"The current function takes as the input a question in the Terminal\"\"\"\n",
    "\n",
    "    print(\"Searching in the vector database for the question {}\".format(question))\n",
    "\n",
    "    # Perform the search (Retrieve)\n",
    "\n",
    "    retrieves = vector_database.similarity_search(question, k=3) # K is the number of chunks the model will retrieve to answer the question.\n",
    "\n",
    "    if not retrieves:\n",
    "        print(\"No relevant information found with the current data stored in my memory. Try adding more data.\")\n",
    "        return\n",
    "    \n",
    "    # Print the chunks the search found after the question\n",
    "\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, retrieve in enumerate(retrieves):\n",
    "        print(f\"Chunk {i+1} extracted from {retrieve.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"and contains the text:\")\n",
    "        print(f\"{retrieve.page_content}\")\n",
    "    \n",
    "    # Combine the retrieved text into one context block\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([retrieve.page_content for retrieve in retrieves])\n",
    "\n",
    "    # Define the prompt to guide the answer (Augment)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant for the novel Re:Zero. \n",
    "    Use the following pieces of retrieved context to answer the user's question.\n",
    "    If the answer is not in the context, just say that you don't know.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask to the LLM (Generate)\n",
    "\n",
    "    print(\"The LLM has been asked the question and is now generating the answer!\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Print the result\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response.content}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce6e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in the vector database for the question Who killed Rom?\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1 extracted from arc-1-chapter-9.txt\n",
      "and have the text Rom’s face was stern as he answered Subaru’s tactless question.\n",
      "\n",
      "He then brought the bottle he had been pouring out of to his mouth, and as he drank,\n",
      "\n",
      "“Because of this, most of us were wiped out. Even in the capital, I haven’t seen any other giants.”\n",
      "\n",
      "“Yer strong even without eatin’, sho kewl. … Gunna throw up.”\n",
      "\n",
      "“I’m saying something sad here and you respond like that?”\n",
      "\n",
      "He wasn’t about to let someone’s sob story kill his mood.\n",
      "\n",
      "As Subaru blocked his ears and interrupted the story, Rom gave up on telling it and started eating his beans.\n",
      "\n",
      "The two of them passed their time silently eating those terrible beans as a side to their alcohol.\n",
      "\n",
      "Eventually there was a coded knock on the door, by which time the sun had already set for the most part.\n",
      "\n",
      "Subaru who had been nodding off raised his head, and Rom nimbly approached the door in response to the sound.\n",
      "Chunk 2 extracted from arc-1-chapter-21.txt\n",
      "and have the text Sighting a rundown shack, she ran up the wall and stood atop it, she then sent spittle flying as she screamed.\n",
      "\n",
      "It wasn’t just anyone that she needed.\n",
      "\n",
      "She needed someone who had the power to face that murderer―Elsa.\n",
      "\n",
      "If anyone would do, she had already seen several figures as she ran here. But Felt was by no means someone well-liked around here.\n",
      "\n",
      "Especially now that she was in trouble, Rom was probably the only one around here who would help her.\n",
      "\n",
      "If she just left Rom like this, he would die.\n",
      "\n",
      "Even Subaru who had helped Felt escape was probably fighting for his life right now.\n",
      "\n",
      "It was a strange feeling. Without understanding why, she desperately rubbed her eyes that were on the verge of tears.\n",
      "\n",
      "Setting Rom aside, what was so bad about some guy she just met dying? He seemed to have had a good upbringing, spoke a lot of nonsense, and to call him inconsiderate was putting it lightly.\n",
      "Chunk 3 extracted from arc-1-chapter-21.txt\n",
      "and have the text He approached Rom’s fallen body, and somehow managed to drag him near the wall.\n",
      "\n",
      "“Old man, Rom. Hey, baldy, listen here, are you still alive?”\n",
      "\n",
      "“Uu… Who’s… Bald…”\n",
      "\n",
      "“You, obviously. As if it’d be anyone else. My only goals in life are to never go bald or become fat. From my perspective, you’re a great example of what not to do.”\n",
      "\n",
      "Though frail, at least he was able to respond, Subaru struck his forehead and heaved a sigh of relief.\n",
      "\n",
      "Setting aside the severe damage to his head, the old man was probably alright. His memory might end up getting a little worse, but that would be a small price to pay to keep his life.\n",
      "\n",
      "“Is that person alright?”\n",
      "\n",
      "As Subaru relaxed, Not-Satella called out while running over to him.\n",
      "\n",
      "With her long silver hair flowing behind her, she analyzed Rom’s injuries, she then whispered, ‘I’ll have to heal this’ as a faint blue glow emanated from her hands.\n",
      "\n",
      "“Hey now, this old man’s one of the people responsible for your insignia’s theft, you know?”\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: Who killed Rom?\n",
      "Answer: Elsa.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk about Arc 1 (or type 'exit'): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        query_question(user_input) # I way to solve the CUDA error is to copy and paste the next line in your CMD: set OLLAMA_USE_CPU=true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
