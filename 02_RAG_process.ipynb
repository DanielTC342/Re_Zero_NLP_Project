{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7624b9e1",
   "metadata": {},
   "source": [
    "# 2. The current Jupyter Notebook will cover the Second Phase of the project \"RAG process\"\n",
    "\n",
    "According to the article \"What is retrieval augmented generation (RAG)?\" by IBM, RAG is \"an architecture for optimizing the performance of an artificial intelligence (AI) model by connecting it with external knowledge bases.\".\n",
    "\n",
    "The mechanics of this architecture are detailed in the seminal paper \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" (Lewis et al., 2020), which demonstrates that it is not necessary to retrain a massive AI model to teach it new facts.\n",
    "\n",
    "## 2.1 Develop the retrieval engine to identify narrative episodes based on vector similarity\n",
    "\n",
    "For this part of the project we will use Ollama to be the server of our RAG system so it will allow us to run LLM locally, therefore python will work as the \"client\".\n",
    "\n",
    "The current code will do the search based on the query and send it to Ollama to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18618ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration to use Ollama correctly created!\n",
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the configuration for the needed to use Ollama\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(\"Configuration to use Ollama correctly created!\")\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")\n",
    "\n",
    "# Connect to Ollama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "# Create a function that performs the vector similarity search\n",
    "\n",
    "def query_question(question):\n",
    "    \"\"\"The current function takes as the input a question in the Terminal\"\"\"\n",
    "\n",
    "    print(\"Searching in the vector database for the question {}\".format(question))\n",
    "\n",
    "    # Perform the search (Retrieve)\n",
    "\n",
    "    retrieves = vector_database.similarity_search(question, k=3) # K is the number of chunks the model will retrieve to answer the question.\n",
    "\n",
    "    if not retrieves:\n",
    "        print(\"No relevant information found with the current data stored in my memory. Try adding more data.\")\n",
    "        return\n",
    "    \n",
    "    # Print the chunks the search found after the question\n",
    "\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, retrieve in enumerate(retrieves):\n",
    "        print(f\"Chunk {i+1} extracted from {retrieve.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"and contains the text:\")\n",
    "        print(f\"{retrieve.page_content}\")\n",
    "    \n",
    "    # Combine the retrieved text into one context block\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([retrieve.page_content for retrieve in retrieves])\n",
    "\n",
    "    # Define the prompt to guide the answer (Augment)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant for the novel Re:Zero. \n",
    "    Use the following pieces of retrieved context to answer the user's question.\n",
    "    If the answer is not in the context, just say that you don't know.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask to the LLM (Generate)\n",
    "\n",
    "    print(\"The LLM has been asked the question and is now generating the answer!\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Print the result\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response.content}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce6e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in the vector database for the question Who helped to defeat Elsa?\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1 extracted from arc-1-chapter-19.txt\n",
      "and contains the text:\n",
      "However, even as she was surrounded by all of this ice, Elsa’s movements now that she had lost her magic-negating cloak had transcended the realm of common sense.\n",
      "\n",
      "She would twist her body, lower herself to the ground as if crawling, and occasionally even defy gravity by running the walls. If she judged that even this wouldn’t suffice, she would strike the ice with her blade, shattering the white crystals. She matched quantity with overwhelming quality, her skill was exceptional.\n",
      "\n",
      "“You handle yourself well for a girl.”\n",
      "\n",
      "Even Puck had to admire her combat sense that could only be described as godlike.\n",
      "\n",
      "His mutter caused Elsa to smile broadly even in the midst of that chaotic battle.\n",
      "\n",
      "“Oh my. It’s been quite a while since I was last regarded as a simple girl.”\n",
      "\n",
      "“Most of my opponents might as well be babies from my perspective. But even so, you’re so strong it’s almost pitiful.”\n",
      "\n",
      "“It’s quite the honor to receive praise from a spirit.”\n",
      "Chunk 2 extracted from arc-1-chapter-16.txt\n",
      "and contains the text:\n",
      "He had died by her hand twice now. She inspired fear from the bottom of his heart, and who could laugh at him for it?\n",
      "\n",
      "Elsa narrowed her eyes like a snake in response to his silence. But even as he felt her gaze pinning him down, he would not allow himself to show weakness by averting his eyes.\n",
      "\n",
      "She licked her lips at his bravado,\n",
      "\n",
      "“… I am somewhat interested, but it’s fine. It wouldn’t do to cause a fuss right now.”\n",
      "\n",
      "“T-That’s not a very nice thing to say. Act too scary and that beauty will go to waste, you know?”\n",
      "\n",
      "“Ah, very nice.― Hide your hostility and it’d be even better.”\n",
      "\n",
      "She reached out and lightly tapped his forehead with her finger, and this simple gesture released the tension from his body.\n",
      "\n",
      "Subaru exhaled, his shoulders heaving, and Elsa placed her finger against her lip,\n",
      "\n",
      "“Well then, I’ll be taking my leave. I get the feeling we’ll see each other again.”\n",
      "\n",
      "“If it’s a bright place with lots of people around, I’ll be able to relax too.”\n",
      "Chunk 3 extracted from arc-1-chapter-10.txt\n",
      "and contains the text:\n",
      "“Keep talking, girl. ―I’ll smash you to pieces and feed you to giant rats!”\n",
      "\n",
      "The club he was swinging to the side certainly had the destructive power he was talking about.\n",
      "\n",
      "Its speed and power were such that any ordinary defense would be ripped apart like paper.\n",
      "\n",
      "There wasn’t much room to move around, and if one were cornered by these blows they would be finished.\n",
      "\n",
      "But on the other side, Elsa’s skill was also at a transcendent level.\n",
      "\n",
      "Still holding her kukri knife with one hand, Elsa’s silhouette seemed to slip right past his maelstrom of attacks.\n",
      "\n",
      "She advanced even as he unleashed attacks that would surely be lethal if they landed, dodging them by a hair. She was practically toying with him as she dodged around.\n",
      "\n",
      "Subaru’s instincts told him this was bad.\n",
      "\n",
      "This is really bad, something within him desperately echoed like an alarm.\n",
      "\n",
      "“Not good…”\n",
      "\n",
      "“It’s okay. Rom couldn’t possibly lose! I’ve never in my life seen him lose, not even once!”\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: Who helped to defeat Elsa?\n",
      "Answer: I don't know. The context doesn't mention anyone helping to defeat Elsa; it actually shows her handling herself well and easily dodging attacks from others, such as Puck and a character referred to as \"Rom\" who is being talked about by Subaru.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk about Arc 1 (or type 'exit'): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        query_question(user_input) # I way to solve the CUDA error is to copy and paste the next line in your CMD: set OLLAMA_USE_CPU=true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
