{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7624b9e1",
   "metadata": {},
   "source": [
    "# 2. The current Jupyter Notebook will cover the Second Phase of the project \"RAG process\"\n",
    "\n",
    "## 2.1 Develop the retrieval engine to identify narrative episodes based on vector similarity\n",
    "\n",
    "For this part of the project we will use Ollama to be the server of our RAG system so it will allow us to run LLM locally, therefore python will work as the \"client\".\n",
    "\n",
    "The current code will do the search based on the query and send it to Ollama to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18618ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lonel\\miniconda3_\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration to use Ollama correctly created!\n",
      "Embedding Model correctly created!\n",
      "Embedding Model correctly connected to the database!\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the configuration for the needed to use Ollama\n",
    "\n",
    "VECTOR_DATABASE_PATH = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\vector_database\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(\"Configuration to use Ollama correctly created!\")\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding Model correctly created!\")\n",
    "\n",
    "# Connect the embedding model to the vector database\n",
    "\n",
    "vector_database = Chroma(persist_directory=VECTOR_DATABASE_PATH,\n",
    "                        embedding_function=embedding_model)\n",
    "print(\"Embedding Model correctly connected to the database!\")\n",
    "\n",
    "# Connect to Ollama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "# Create a function that performs the vector similarity search\n",
    "\n",
    "def query_question(question):\n",
    "    \"\"\"The current function takes as the input a question in the Terminal\"\"\"\n",
    "\n",
    "    print(\"Searching in the vector database for the question {}\".format(question))\n",
    "\n",
    "    # Perform the search (Retrieve)\n",
    "\n",
    "    retrieves = vector_database.similarity_search(question, k=3) # K is the number of chunks the model will retrieve to answer the question.\n",
    "\n",
    "    if not retrieves:\n",
    "        print(\"No relevant information found with the current data stored in my memory. Try adding more data.\")\n",
    "        return\n",
    "    \n",
    "    # Print the chunks the search found after the question\n",
    "\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, retrieve in enumerate(retrieves):\n",
    "        print(f\"Chunk {i+1}: {retrieve.page_content}\")\n",
    "    \n",
    "    # Combine the retrieved text into one context block\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([retrieve.page_content for retrieve in retrieves])\n",
    "\n",
    "    # Define the prompt to guide the answer (Augment)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant for the novel Re:Zero. \n",
    "    Use the following pieces of retrieved context to answer the user's question.\n",
    "    If the answer is not in the context, just say that you don't know.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask to the LLM (Generate)\n",
    "\n",
    "    print(\"The LLM has been asked the question and is now generating the answer!\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Print the result\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response.content}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce6e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in the vector database for the question Is Satella Emilia?\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1: That being the case, she was definitely linked to his death. If things were like the second time, it was very likely that Felt’s greed caused negotiations to break down.\n",
      "\n",
      "“She pointlessly provoked her and ended up getting killed for it, maybe.”\n",
      "\n",
      "Considering her extremely competitive spirit, it was certainly possible.\n",
      "\n",
      "And right as Elsa had silenced them, Subaru and Satella arrived with terrible timing. This was probably how the first time went.\n",
      "\n",
      "“The second time was quite a bit simpler, wasn’t it. I just happened to be one of the people being silenced.”\n",
      "\n",
      "Thinking of it this way, perhaps Satella ended up arriving there after they got murdered.\n",
      "\n",
      "He knew about her magical ability, but he doubted that that homicidal maniac would give her time to chant. Satella was probably at a disadvantage 8 or 9 times out of 10.\n",
      "\n",
      "“Wait, I’ve been killed by the same person twice. Simply put, Elsa’s basically a certain death encounter, final answer. Also…”\n",
      "Chunk 2: They were all shaken and had fallen silent, as though even the slightest movement was forbidden.\n",
      "\n",
      "It was as if the conversation between the two of them had completely dominated the street.\n",
      "\n",
      "“I’m asking you what you’re thinking. Speak up.”\n",
      "\n",
      "However, Satella wouldn’t allow any hesitation despite his bewilderment.\n",
      "\n",
      "Her tone was harsh, but Subaru had no response since he didn’t even know why she was angry.\n",
      "\n",
      "They seemed to be on completely different wavelengths as to what the problem was.\n",
      "\n",
      "“I’ll ask this once more. ―Why did you refer to me by the name of the Witch of Envy?”\n",
      "\n",
      "“No, I mean, I was told…”\n",
      "\n",
      "“I don’t know who told you that, but that’s in very poor taste. You’re at fault too for going along with it. ―The Witch of Envy, the symbol of all things taboo, merely mentioning her name is frowned upon, to think you’d actually call someone by that name.”\n",
      "\n",
      "Her anger clearly on display, Satella―The silver-haired girl plunged him into a sea of confusion.\n",
      "Chunk 3: Suddenly clenching his fist, he spoke his earlier worry out loud and laughed it off.\n",
      "\n",
      "The conditions all connected properly. It was impossible that Satella didn’t actually exist.\n",
      "\n",
      "He would definitely meet her again once he’d obtained the insignia.\n",
      "\n",
      "As Subaru strengthened his resolve, Rom watched him with eyes as though he was looking at something extremely strange.\n",
      "\n",
      "“―You really are an idiot, huh.”\n",
      "\n",
      "With this, all other feelings were forgotten and all that was left was laughter.\n",
      "The LLM has been asked the question and is now generating the answer!\n",
      "\n",
      "==============================\n",
      "Question: Is Satella Emilia?\n",
      "Answer: I don't know. The text mentions \"The silver-haired girl\" and refers to her as Satella, but it does not confirm whether she is indeed the person known as Emilia.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk about Arc 1 (or type 'exit'): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        query_question(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
