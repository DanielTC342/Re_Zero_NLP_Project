{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae313ee",
   "metadata": {},
   "source": [
    "# The current Jupyter Notebook will cover the full life cylce of the First Phase of the project: ETL process and \"memory building\"\n",
    "\n",
    "## Scrape and extract textual content\n",
    "\n",
    "In this step we will extract the needed data from the \"Witch Cult Translations\" site.\n",
    "\n",
    "Because every ARC is divided into n characters, it is necessary to loop the main page to extract the text of every chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries for the step\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the object of BeautifulSoup\n",
    "URL = \"https://witchculttranslation.com/table-of-content/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Define the \"route\" of where the table of contents is saved on the main page\n",
    "\n",
    "principal_container = soup.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "# Define the \"route\" where the links of every chapter are saved\n",
    "\n",
    "chapters_links = principal_container.find_all(\"a\")\n",
    "\n",
    "# Extract all the URLs found\n",
    "\n",
    "chapters_urls = [] # Use to save the URLs of the chapters\n",
    "\n",
    "for link in chapters_links:\n",
    "\n",
    "    chapter_link = link['href']\n",
    "\n",
    "    chapters_urls.append(chapter_link)\n",
    "\n",
    "## Optimized version of the code above\n",
    "## chapters_urls = [link['href'] for link in chapters_links]\n",
    "\n",
    "# The urls of the chapters follows the next \"pattern\": https://witchculttranslation.com/aaaa/mm/dd/arc-n-chapter-n-title/\n",
    "# So it is a good idea to filter the extracted ULRs by the word \"arc\" so we avoid all the \"unnecessary\" URLs.\n",
    "\n",
    "cleaned_chapters_urls = []\n",
    "\n",
    "for url in chapters_urls:\n",
    "    if \"arc\" in url:\n",
    "        cleaned_chapters_urls.append(url)\n",
    "\n",
    "\n",
    "# Loop \n",
    "\n",
    "for url in cleaned_chapters_urls:\n",
    "\n",
    "    # Download the page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup_parser = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Find the text container\n",
    "    text_container = soup.find(\"div\", class_=\"entry_content\")\n",
    "\n",
    "    # Define a counter\n",
    "    n = 0\n",
    "\n",
    "    # Extract the text\n",
    "    if text_container:\n",
    "        chapter_text = text_container.get_text(separator=\"\\n\\n\", strip=True)\n",
    "        n += 1\n",
    "\n",
    "        with open(f\"chapter_{n}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chapter_text)\n",
    "            \n",
    "    else:\n",
    "        print(\"Text not found. Review your selector.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
