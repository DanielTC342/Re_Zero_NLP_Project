{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae313ee",
   "metadata": {},
   "source": [
    "# The current Jupyter Notebook will cover the full life cylce of the First Phase of the project: ETL process and \"memory building\"\n",
    "\n",
    "## Scrape and extract textual content\n",
    "\n",
    "In this step we will extract the needed data from the \"Witch Cult Translations\" site.\n",
    "\n",
    "Because every ARC is divided into n characters, it is necessary to loop the main page to extract the text of every chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3a7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 23 chapters...\n",
      "File arc-1-chapter-1.txt saved correctly!\n",
      "File arc-1-chapter-2.txt saved correctly!\n",
      "File arc-1-chapter-3.txt saved correctly!\n",
      "File arc-1-chapter-4.txt saved correctly!\n",
      "File arc-1-chapter-5.txt saved correctly!\n",
      "File arc-1-chapter-6.txt saved correctly!\n",
      "File arc-1-chapter-7.txt saved correctly!\n",
      "File arc-1-chapter-8.txt saved correctly!\n",
      "File arc-1-chapter-9.txt saved correctly!\n",
      "File arc-1-chapter-10.txt saved correctly!\n",
      "File arc-1-chapter-11.txt saved correctly!\n",
      "File arc-1-chapter-12.txt saved correctly!\n",
      "File arc-1-chapter-13.txt saved correctly!\n",
      "File arc-1-chapter-14.txt saved correctly!\n",
      "File arc-1-chapter-15.txt saved correctly!\n",
      "File arc-1-chapter-16.txt saved correctly!\n",
      "File arc-1-chapter-17.txt saved correctly!\n",
      "File arc-1-chapter-18.txt saved correctly!\n",
      "File arc-1-chapter-19.txt saved correctly!\n",
      "File arc-1-chapter-20.txt saved correctly!\n",
      "File arc-1-chapter-21.txt saved correctly!\n",
      "File arc-1-chapter-22.txt saved correctly!\n",
      "Errror downloading https://witchculttranslation.com/2021/05/19/arc-1-interlude-the-moon-is-watching/: 'NoneType' object has no attribute 'group'\n",
      "Download completed!\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries for the step\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define the object of BeautifulSoup\n",
    "URL = \"https://witchculttranslation.com/table-of-content/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Define the \"route\" of where the table of contents is saved on the main page\n",
    "\n",
    "principal_container = soup.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "# Define the \"route\" where the links of every chapter are saved\n",
    "\n",
    "chapters_links = principal_container.find_all(\"a\")\n",
    "\n",
    "# Extract all the URLs found\n",
    "\n",
    "chapters_urls = [] # Use to save the URLs of the chapters\n",
    "\n",
    "for link in chapters_links:\n",
    "\n",
    "    chapter_link = link.get('href')\n",
    "\n",
    "    chapters_urls.append(chapter_link)\n",
    "\n",
    "## Optimized version of the code above\n",
    "## chapters_urls = [link['href'] for link in chapters_links]\n",
    "\n",
    "# The urls of the chapters follows the next pattern (at least in the first chapter):\n",
    "# https://witchculttranslation.com/aaaa/mm/dd/arc-n-chapter-n-title/\n",
    "# So it is a good idea to filter the extracted ULRs by the word \"arc-1\" so we avoid all the \"unnecessary\" URLs.\n",
    "\n",
    "cleaned_chapters_urls = []\n",
    "\n",
    "for url in chapters_urls:\n",
    "    # If we want to extract the url of all the chapters of all acrs,\n",
    "    # instead of using \"arc-1\" we should use just \"arc\"\n",
    "    if \"arc-1\" in url:\n",
    "        cleaned_chapters_urls.append(url)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Loop\n",
    "\n",
    "print(f\"Starting download of {len(cleaned_chapters_urls)} chapters...\")\n",
    "\n",
    "for url in cleaned_chapters_urls:\n",
    "\n",
    "    try:\n",
    "        # Add a timer to avoid a ban from the server\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Download the page\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        page = requests.get(url, headers=headers)\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup_parser = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Find the text container\n",
    "        text_container = soup_parser.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "        # Extract the text\n",
    "        if text_container:\n",
    "            chapter_text = text_container.get_text(separator=\"\\n\\n\", strip=True)\n",
    "\n",
    "            # Save the text of the files\n",
    "            import re\n",
    "\n",
    "            # Define the regular expression to download only the chapters of the first arc.\n",
    "            # Due to the links pattern to name the URL of the first arc of the novel.\n",
    "\n",
    "            match = re.search(r'arc-1-chapter-\\d+', url)\n",
    "\n",
    "            filename = f\"{match.group(0)}.txt\"\n",
    "\n",
    "            # To extract more data to feed the model with in future phases of the project we can use the next code:\n",
    "            # in order to extract the arc and chapter of the links\n",
    "            # arc_match = re.search(r'arc-\\d+', url)\n",
    "            # chapter_match = re.search(r'chapter-\\d+', url)\n",
    "            # filename = f\"{arc_match.group(0)}{chapter_match.group(0)}.txt\"\n",
    "\n",
    "            # Import os to save the data in a specific folder\n",
    "            import os\n",
    "\n",
    "            folder = r\"C:\\Users\\lonel\\OneDrive\\Escritorio\\Re Zero NLP Project\\chapters_files\"\n",
    "\n",
    "            full_path = os.path.join(folder, filename)\n",
    "\n",
    "            with open(full_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(chapter_text)\n",
    "            print(f\"File {filename} saved correctly!\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Text not found in {url}. Review your selector.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Errror downloading {url}: {e}\")\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
